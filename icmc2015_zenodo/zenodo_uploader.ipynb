{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "ACCESS_TOKEN = ''\n",
    "\n",
    "papers = [\n",
    "    {\n",
    "        'file': 'Batchelor.pdf',\n",
    "        'title': 'ChuckSound: A Chugin for Running Csound Inside of ChucK',\n",
    "        'authors': [('Paul Batchelor', '', 'thisispaulbatchelor@gmail.com')],\n",
    "        'abstract': 'ChuckSound is a plugin for ChucK (otherwise known as a “chugin”) that allows Csound to be run inside of ChucK. Prior to ChuckSound, a typical setup for getting Csound + Chuck working together would be to start ChucK and Csound as separate applications, and to use OSC and/or JACK to communicate. With ChuckSound, Csound is spawned inside of ChucK’s audio engine via the Csound API. This approach allows Csound to work seamlessly with ChucK objects without any sort of latency that OSC would produce. ChuckSound has the ability to evaluate Csound orchestra code inside of ChucK as well as send score events.'\n",
    "    }, {\n",
    "        'file': 'Brandtsegg_Tidemann.pdf',\n",
    "        'title': 'On Audio Processes in the Artificial Intelligence [self.]',\n",
    "        'authors': [\n",
    "            ('Øyvind Brandtsegg', 'Norwegian University of Science and Technology, Trondheim, Norway', 'oyvind.brandtsegg@ntnu.no'),\n",
    "            ('Axel Tidemann', 'Norwegian University of Science and Technology, Trondheim, Norway', 'tidemann@idi.ntnu.no')],\n",
    "        'abstract': 'This paper describes [self.], an open source art installation that embodies artificial intelligence (AI) in order to learn, react, and respond to stimuli from its immediate environment. Biologically inspired models are implemented to achieve this behavior, and Csound is used for most parts of the audio processing involved in the system. The artificial intelligence is physically represented by a robot head, built on a modified moving head for stage lighting. Everything but the motors of the stage lighting unit was removed and a projector, camera and microphones added. No form of knowledge or grammar have been implemented in the AI, the system starts in a ``tabula rasa'' state and learns everything via its own sensory channels, forming categories in a bottom-up fashion. The robot recognizes sounds and faces, and is able to recognize similar sounds, link them with the corresponding faces, and use the knowledge of past experiences to form new sentences. Since the utterances of the AI is solely based on audio and video items it has learned from the interaction with people, an insight into the learning process (i.e. what it has learned from who) can be glimpsed. This collage-like composition has guided several design choices regarding the aesthetics of the audio and video output. This paper will focus on the audio processes of the system, herein audio recording, segmentation, analysis, processing and playback.'\n",
    "    }, {\n",
    "        'file': 'Cabrera.pdf',\n",
    "        'title': 'Introducing icsound: Interactive Sonification with Csound',\n",
    "        'authors': [('Andrés Cabrera', '', 'andres@mat.ucsb.edu')],\n",
    "        'abstract': 'Auditory displays present information through sound. As part of an auditory display, the process of rendering information and interaction as sound is called sonification. Sonification can take many forms and be applied to many different problems: from understanding radiation through the clicks from a Geiger counter to developing the complex sound language presenting information in some computer games today. The study of sonification is very developed and a scientific community with expertise in sound synthesis, big data, user interaction, computer science and cognition (among others!) has gathered together around it [1].'\n",
    "    }, {\n",
    "        'file': 'ffitch_Brain.pdf',\n",
    "        'title': 'Use of Multiple Cores in Csound',\n",
    "        'authors': [('John ffitch', 'National University of Ireland, Maynooth', ''),\n",
    "                  ('Martin Brain', 'University of Oxford', '')],\n",
    "        'abstract': 'The basic concepts and the new algorithms that are used in Csound6 in order to enhance performance via parallelism are described in some detail. The hope is that this will assist users in determining when this technology is advantageous, and in how to write instruments that are suited to this level of parallelism. The paper ends with a short consideration of how the scheme could be improved.'\n",
    "    }, {\n",
    "        'file': 'Giordani_Petrolati.pdf',\n",
    "        'title': 'Csound Synthesis Approach for the iVCS3 iOS App',\n",
    "        'authors': [('Eugenio Giordani', 'LEMS Laboratorio Elettronico per la Musica Sperimentale Conservatory of Music G. Rossini – Pesaro - Italy', ''),\n",
    "                  ('Alessandro Petrolati', 'apeSoft', '')],\n",
    "        'abstract': 'This article is about the process of creating an iOS based app for the digital emulation of the famous electronic synthesizer VCS3 by EMS using a Csound orchestra as its sound engine. Despite the out of standard features of the original instrument, we have attempted to clone a large amount of details regarding the sound generation, the connectivity, the look and its specific original ergonomy. We will try to illustrate the general synthesis strategies adopted in creating this musical synth application and in particular, we will focus on Csound code from the main sound modules and their relative individual modeling approach.'\n",
    "    }, {\n",
    "        'file': 'Gogins.pdf',\n",
    "        'title': 'Expanding the Power of Csound with Integrated HTML and JavaScript',\n",
    "        'authors': [('Michael Gogins', '', 'michael.gogins@gmail.com')],\n",
    "        'abstract': 'This paper presents recent developments integrating Csound [1] with HTML [2] and JavaScript [3, 4]. For those new to Csound, it is a “MUSIC N” style, user- programmable software sound synthesizer, one of the first yet still being extended, written mostly in the C language. No synthesizer is more powerful. Csound can now run in an interactive Web page, using all the capabilities of current Web browsers: custom widgets, 2- and 3-dimensional animated and interactive graphics canvases, video, data storage, WebSockets, Web Audio, mathematics typesetting, etc. See the whole list at HTML5 TEST [5]. Above all, the JavaScript programming language can be used to control Csound, extend its capabilities, generate scores, and more. JavaScript is the “glue” that binds together the components and capabilities of HTML5. JavaScript is a full-featured, dynamically typed language that supports functional programming and prototype-based object- oriented programming. In most browsers, the JavaScript virtual machine includes a just- in-time compiler that runs about 4 times slower than compiled C, very fast for a dynamic language. JavaScript has limitations. It is single-threaded, and in standard browsers, is not permitted to access the local file system outside the browser\\'s sandbox. But most musical applications can use an embedded browser, which bypasses the sandbox and accesses the local file system.'\n",
    "    }, {\n",
    "        'file': 'Heintz.pdf',\n",
    "        'title': 'Knuth and Alma: Two Partners for Live-Electronics with Spoken Word',\n",
    "        'authors': [('Joachim Heintz', '', 'joachim.heintz@hmtm-hannover.de')],\n",
    "        'abstract': 'The main idea behind Knuth and Alma is the development of a simple live-electronic setup for a speaker. It follows the german child song \"Ich geh mit meiner Laterne\", which describes a couple of a child and a lantern. For the speaker, their lantern is a small loudspeaker; small enough to be carried and to be put on the table, beneath the speaker when they read a text, large enough to be a counterpart to the human voice. Although the setup is the same for both, Knuth and Alma focus on quite different aspects of spoken language, and output quite different sounds. Knuth analyzes the rhythm of the language and triggers pre-recorded samples of any sound, whereas Alma recalls parts of the speaker\\'s past and does not use any other sound except the speaker\\'s live input itself. I will first describe the two models and their implementation in Csound, and then I will discuss some use cases and future possibilities.'\n",
    "    }, {\n",
    "        'file': 'ICSC2015 Cover_Foreword_Contents.pdf',\n",
    "        'title': 'Foreword',\n",
    "        'authors': [('Gleb G. Rogozinsky', 'The Bonch-Bruevich Saint-Petersburg State University of Telecommunications, St. Petersburg, Russia', 'gleb.rogozinsky@gmail.com')],\n",
    "        'abstract': 'A foreword to the International Csound Conference 2015 proceedings'\n",
    "    }, {\n",
    "        'file': 'Jordan-Kamholz_Boulanger.pdf',\n",
    "        'title': 'Bringing Csound to a Modern Production Environment With Csound for Live',\n",
    "        'authors': [('Mark Jordan-Kamholz', 'Berklee College of Music', 'mjordankamholz@berklee.edu'),\n",
    "                  ('Dr. Richard Boulanger', 'Berklee College of Music', 'rboulanger@berklee.edu')],\n",
    "        'abstract': 'Csound is a powerful and versatile synthesis and signal processing system and yet, it has been far from convenient to use the program in tandem with a modern Digital Audio Workstation (DAW) setup. While it is possible to route MIDI to Csound, and audio from Csound, there has never been a solution that fully integrated Csound into a DAW. Csound for Live attempts to solve this problem by using csound~, Max and Ableton Live. Over the course of this paper, we will discuss how users can use Csound for Live to create Max for Live Devices for their Csound instruments that allow for quick editing via a GUI; tempo-synced and transport-synced operations and automation; the ability to control and receive information from Live via Ableton’s API; and how to save and recall presets. In this paper, the reader will learn best practices for designing devices that leverage both Max and Live, and in the presentation, they will see and hear demonstrations of devices used in a full song, as well as how to integrate the powerful features of Max and Live into a production workflow.'\n",
    "    }, {\n",
    "        'file': 'Kholomiov.pdf',\n",
    "        'title': 'Csound Accelerated with Haskell: Introducing the Library Csound-expression',\n",
    "        'authors': [('Anton Kholomiov', '', 'anton.kholomiov@gmail.com')],\n",
    "        'abstract': 'Csound is a very powerful audio engine, but by syntactic ease of use it is still far behind the modern languages like Python, Ruby or Haskell. That is sad because clumsy syntax prevents many users from unlocking the real powers of Csound. In my library csound- expresion I\\'d like to take the best parts of Csound (powerful audio engine and scheduler) and supply them with the wings of syntax gifted to Haskell. The proposed solution can greatly enhance the productivity of the Csound musician.'\n",
    "    }, {\n",
    "        'file': 'Rogozinsky_Cherny_Osipenko.pdf',\n",
    "        'title': 'Making Mainstream Synthesizers with Csound',\n",
    "        'authors': [('Gleb G. Rogozinsky', 'The Bonch-Bruevich Saint-Petersburg State University of Telecommunications, St. Petersburg, Russia', 'gleb.rogozinsky@gmail.com'),\n",
    "                  ('Eugene Cherny', 'Åbo Akademi University, Turku, Finland', 'eugene.cherny@oscii.ru'),\n",
    "                  ('Ivan Osipenko', 'Saber Interactive, St. Petersburg, Russia', '')],\n",
    "        'abstract': 'For more than the past twenty years, Csound has been one of the leaders in the world of the computer music research, implementing innovative synthesis methods and making them available beyond the academic environments from which they often arise, and into the hands of musicians and sound designers throughout the world. In its present state, Csound offers an efficient environment for sound experimentation, allowing the user to work with almost any known sound synthesis or signal processing method through its vast collection of ready-made opcodes. But despite all this potential, the shared resource of Csound instruments still lacks quality reproductions of well-known synthesizers; even with its ability to generate commercial standard user interfaces and with the possibility to compile Csound instruments in such as fashion so that they can be used with no knowledge of Csound code. To fill this gap, the authors have implemented two commercial-style synthesizers as VST plug-ins using the Csound front-end ‘Cabbage’. This paper describes their architecture and some of the Csound specific challenges involved in the development of fully featured synthesizers.'\n",
    "    }, {\n",
    "        'file': 'Rogozinsky_Chesnokov.pdf',\n",
    "        'title': 'Clavia Nord Modular G2 Patch Converter Project',\n",
    "        'authors': [('Gleb G. Rogozinsky', 'The Bonch-Bruevich Saint-Petersburg State University of Telecommunications, St. Petersburg, Russia', 'gleb.rogozinsky@gmail.com'),\n",
    "                  ('Michael Chesnokov', 'Saint-Petersburg State Institute of Film and Television, St. Petersburg, Russia', 'chesnokov.inc@gmail.com')],\n",
    "        'abstract': 'One of most remarkable hardware synthesizers of the late 90s, the Clavia Nord Modular G2 was discontinued in 2009. It inspired a whole new generation of modular synthesizer fans by combining the immediacy of dedicated hardware with the power and flexibility of computer-based programming. The suite of components comprising the G2 project included an extensive list of modules (from oscillators to effects), an attractive software graphical interface and on the hardware side, keyboard and rack options. The website Vintage Synth Explorer rates the G2 as “awesome” and it has been used extensively by artists such as Astral Projection, Autechre, The Chemical Brothers, Somatic Responses, Junkie XL, Mouse on Mars, Nine Inch Nails and Covenant amongst others. What really makes the G2 unique and sustains its relevance is the rich archive of patches that exists. The NMG2 community remains active, publishing new patches ranging from sound synthesizers to algorithmic generators. It is difficult to compare this synth to commercial synths like the Access Virus or Roland JP8k, but its remarkable features are algorithmic patches and drones. Alongside their inherent use to musicians, NMG2 patches can also incorporate a valuable educational aspect, inspiring people to develop their own patching skills.'\n",
    "    }, {\n",
    "        'file': 'Sigurdsson.pdf',\n",
    "        'title': 'Live Coding and Csound',\n",
    "        'authors': [('Hlöðver Sigurðsson', '', 'hlolli@gmail.com')],\n",
    "        'abstract': 'In this paper I\\'m going to cover a way to design a live-coding front end for Csound. My programming language of choice is Clojure, which I\\'ve used to develop Panaeolus, a live-coding program built with CsoundAPI. The aim of Panaeolus is not only to bring Csound into the world of functional programming and lisp, but also to build an extendable musical system that can create complex musical pattern, with as short and descriptive code as possible. The origin of Panaeolus dates back to April 2015 when I was using Overtone (SupercolliderAPI for Clojure) for live-coding. Initially I just added few Csound instruments into my live-coding sets, but as my preference for the acoustical qualities of Csound are greater than those of Supercollider, I decided to leave the world of Supercollider and began to develop my own live-coding environment in July that same year. At the time of this writing, Panaeolus still needs better documentation, testing and stable release. It can be found under GNU license on http://github.com/hlolli/panaeolus. Even tough I will explain concepts in this paper that apply to Clojure, I will to point out that almost identical principles apply to other programming languages, even the Csound language itself. And at the time of this writing, a short article of live-coding in the Csound language with CsoundQt front-end is scheduled for Csound Journal spring issue of 2016.'\n",
    "    }, {\n",
    "        'file': 'Toshihiro.pdf',\n",
    "        'title': 'System Configuration for an Audience-Driven Piece Including Sonification and Visualization of Audience\\'s Operations',\n",
    "        'authors': [('Kita Toshihiro', '', 'http://tkita.net')],\n",
    "        'abstract': 'Recent penetration of smartphones has made audience participation in live electronics pieces easily possible. The piece played by the author at Csound Conference 2013 is based on the data sent from audience by using their own smartphones or tablets without having to install any dedicated app. The system uses a WiFi router for accepting connection from each participant, and a Linux laptop that runs Csound and lighttpd. By performing XMLHttpRequest data transmissions from the smartphone browsers, OSC message is generated by a PHP script to control Csound. This paper describes the system configuration for the piece. The revised system that additionally includes processing by HTML5 Web Audio API and Websocket that were added to enhance feedback to each participant is also described.'\n",
    "    }, {\n",
    "        'file': 'Vinay_Boulanger.pdf',\n",
    "        'title': 'Building Web Based Interactive Systems with Csound PNaCl and WebSockets',\n",
    "        'authors': [('Ashvala Vinay', 'Berklee College of Music', 'avinay@berklee.edu'),\n",
    "                  ('Dr. Richard Boulanger', 'Berklee College of Music', 'rboulanger@berklee.edu')],\n",
    "        'abstract': 'This project aims to harness WebSockets to build networkable interfaces and systems using Csound’s Portable Native Client binary (PNaCl) and Socket.io. There are two methods explored in this paper. The first method is to create an interface to Csound PNaCl on devices that are incapable of running the Native Client binary. For example, by running Csound PNaCl on a desktop and controlling it with a smartphone or a tablet. The second method is to create an interactive music environment that allows users to run Csound PNaCl on their computers and use musical instruments in an orchestra interactively. In this paper, we will also address some of the practical problems that exist with modern interactive web-based/networked performance systems – latency, local versus global, and everyone controlling every instrument versus one person per instrument. This paper culminates in a performance system that is robust and relies on web technologies to facilitate musical collaboration between people in different parts of the world.'\n",
    "    }, {\n",
    "        'file': 'Walsh.pdf',\n",
    "        'title': 'Csound and Unity3D',\n",
    "        'authors': [('Rory Walsh', 'Ireland', '')],\n",
    "        'abstract': 'Adaptive audio for games presents a serious challenge for both developers and sound designers. Existing audio middleware offer some solutions, but fall way short of the kind of tools sound designers are used to working with. This text will focus on the use of Csound as a fully integrated sound engine for the Unity(3D) game engine. It should be noted that this text is written from the perspective of a musician and sound designer rather than a game developer. Nonetheless, it does try to cover as much of the core principles of game design in Unity as is possible within the restrictions of a single text. As the main Csound wrapper for Unity is written in C#, this is the language that all scripting examples will be shown in. Readers should have some basic understanding of Csound and C#.'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_metadata(paper_json: dict) -> dict:\n",
    "    return {\n",
    "        'upload_type': 'publication',\n",
    "        'publication_type': 'conferencepaper',\n",
    "        'publication_date': '2016-03-01',\n",
    "        'title': paper_json['title'],\n",
    "        'creators': [{'name': n, 'affiliation': a} for n,a,e in paper_json['authors']],\n",
    "        'description': paper_json['abstract'],\n",
    "        'access_right': 'open',\n",
    "        'license': 'cc-by-sa',\n",
    "        'conference_title': 'The Third International Csound Conference',\n",
    "        'conference_acronym': 'ICSC2015',\n",
    "        'conference_dates': '2-4 October 2015',\n",
    "        'conference_place': 'St. Petersburg, Russia',\n",
    "        'conference_url': 'http://csound.github.io/icsc2015/',\n",
    "        'imprint_publisher': 'The Bonch-Bruevich St. Petersburg State University of Telecommunications',\n",
    "        'imprint_isbn': '978-5-89160-124-6',\n",
    "        'imprint_place': 'St. Petersburg, Russia',\n",
    "        'partof_title': 'Proceedings of the Third International Csound Conference',\n",
    "        'partof_pages':  '155',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p in papers:\n",
    "    print('Paper: ' + p['title'])\n",
    "    r = requests.post('https://zenodo.org/api/deposit/depositions?access_token=' + ACCESS_TOKEN,\n",
    "                      data=\"{}\", headers=headers)\n",
    "    print(\"  step 1: \" + str(r.status_code))\n",
    "    did = r.json()['id']\n",
    "    data = {'filename': p['file']}\n",
    "    files = {'file': open(p['file'], 'rb')}\n",
    "    r = requests.post((\"https://zenodo.org/api/deposit/depositions/{}/files?access_token=\" + ACCESS_TOKEN).format(did),\n",
    "                      data=data, files=files)\n",
    "    print(\"  step 2: \" + str(r.status_code))\n",
    "    data = {'metadata': create_metadata(p)}\n",
    "    r = requests.put((\"https://zenodo.org/api/deposit/depositions/{}?access_token=\" + ACCESS_TOKEN).format(did),\n",
    "                      data=json.dumps(data), headers=headers)\n",
    "    print(\"  step 3: \" + str(r.status_code))\n",
    "    r = requests.post((\"https://zenodo.org/api/deposit/depositions/{}/actions/publish?access_token=\" + ACCESS_TOKEN).format(did))\n",
    "    print(\"  step 4: \" + str(r.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
